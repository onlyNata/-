I found that in ./utils/object_detection_evaluation.py has already calculated the PR value by the compute_precision_recall function in ./utils/metrics.py.

The detail of how to evaluate the detector is contained in ClassObjectDetectionEvaluation and Funcevaluate(). It returns the pr curve to the interface class ObjectDetectionEvaluator Func evaluate(). So if you do not need the TP,FP values, Func evaluate() can be change like this:

./utils/object_detection_evaluation.py class ObjectDetectionEvaluator Func evaluate()

  def evaluate(self):
    """Compute evaluation result.

    Returns:
      A dictionary of metrics with the following fields -

      1. summary_metrics:
        'Precision/mAP@<matching_iou_threshold>IOU': mean average precision at
        the specified IOU threshold.

      2. per_category_ap: category specific results with keys of the form
        'PerformanceByCategory/mAP@<matching_iou_threshold>IOU/category'.
    """
    (per_class_ap, mean_ap, precisions_per_class, recalls_per_class, per_class_corloc, mean_corloc) = (
        self._evaluation.evaluate())
    pascal_metrics = {
        self._metric_prefix +
        'Precision/mAP@{}IOU'.format(self._matching_iou_threshold):
            mean_ap
    }
    pr_value = {}
    if self._evaluate_corlocs:
      pascal_metrics[self._metric_prefix + 'Precision/meanCorLoc@{}IOU'.format(
          self._matching_iou_threshold)] = mean_corloc
    category_index = label_map_util.create_category_index(self._categories)
    for idx in range(per_class_ap.size):
      if idx + self._label_id_offset in category_index:
        display_name = (
            self._metric_prefix + 'PerformanceByCategory/AP@{}IOU/{}'.format(
                self._matching_iou_threshold,
                category_index[idx + self._label_id_offset]['name']))
        pascal_metrics[display_name] = per_class_ap[idx]


        # PR curve
        display_name = (
            'PR_curve@{}'.format(category_index[idx + self._label_id_offset]['name']))
        pr_value[display_name] = {'precisions': precisions_per_class[idx], 'recalls': recalls_per_class[idx]}

        # Optionally add CorLoc metrics.classes
        if self._evaluate_corlocs:
          display_name = (
              self._metric_prefix + 'PerformanceByCategory/CorLoc@{}IOU/{}'
              .format(self._matching_iou_threshold,
                      category_index[idx + self._label_id_offset]['name']))
          pascal_metrics[display_name] = per_class_corloc[idx]

    return pascal_metrics, pr_value
Notice that the precisions_per_class and recalls_per_class returns the pr value per class. And we return all the PR_value with in a new object pr_value.

Then we change the interface of func _run_checkpoint_once and func repeated_checkpoint_run in ./eval_util.py as following:

./eval_util.py func _run_checkpoint_once

for evaluator in evaluators:
        #metrics = evaluator.evaluate()
        metrics, pr_value = evaluator.evaluate()
....

#return (global_step, all_evaluator_metrics)
return (global_step, all_evaluator_metrics, pr_value)
./eval_util.py func repeated_checkpoint_run

        # global_step, metrics = _run_checkpoint_once_change(tensor_dict, evaluators,
        #                                             batch_processor,
        #                                             last_evaluated_model_path,
        #                                             variables_to_restore,
        #                                             None, num_batches,
        #                                             master, save_graph,
        #                                             save_graph_dir)
        global_step, metrics, pr_value = _run_checkpoint_once_change(tensor_dict, evaluators,
                                                    batch_processor,
                                                    last_evaluated_model_path,
                                                    variables_to_restore,
                                                    None, num_batches,
                                                    master, save_graph,
                                                    save_graph_dir)
Then, change the function write_metrics() in file ./eval_util.py to write the pr_curve.

./eval_util.py func write_pr_curve()

def write_metrics(metrics, global_step, summary_dir, pr_value=None):
  """Write metrics to a summary directory.

  Args:
    metrics: A dictionary containing metric names and values.
    global_step: Global step at which the metrics are computed.
    summary_dir: Directory to write tensorflow summaries to.
    pr_value: A dictionary containing pr_value names and values.
  """
  logging.info('Writing metrics to tf summary.')
  summary_writer = tf.summary.FileWriter(summary_dir)
  for key in sorted(metrics):
    summary = tf.Summary(value=[
        tf.Summary.Value(tag=key, simple_value=metrics[key]),
    ])
    summary_writer.add_summary(summary, global_step)
    logging.info('%s: %f', key, metrics[key])
  if pr_value is not None:
    for key in sorted(pr_value):
      # this part is used to control the num of the points in the PR curve, 
      # since it always generates much more than 10,000 points in the pr curve, which is not necessary to plot all these point into the PR curve.
      num_thresholds = min(500, len(pr_value[key]['precisions']))
      if num_thresholds != len(pr_value[key]['precisions']):
        gap = len(pr_value[key]['precisions']) / num_thresholds
        pr_value[key]['precisions'] = np.append(pr_value[key]['precisions'][::gap], pr_value[key]['precisions'][-1])
        pr_value[key]['recalls'] = np.append(pr_value[key]['recalls'][::gap], pr_value[key]['recalls'][-1])
        num_thresholds = len(pr_value[key]['precisions'])
      # the pr_curve_raw_data_pb() needs the a ascending precisions array and a descending recalls array
      pr_value[key]['precisions'].sort()
      pr_value[key]['recalls'][::-1].sort()
      #write pr curve
      summary = summary_lib.pr_curve_raw_data_pb(
        name=key,
        true_positive_counts=-np.ones(num_thresholds),
        false_positive_counts=-np.ones(num_thresholds),
        true_negative_counts=-np.ones(num_thresholds),
        false_negative_counts=-np.ones(num_thresholds),
        precision=pr_value[key]['precisions'],
        recall=pr_value[key]['recalls'],
        num_thresholds=num_thresholds
        )
      summary_writer.add_summary(summary, global_step)
      logging.info('%s: %f', key, pr_value[key])
  summary_writer.close()
  logging.info('Metrics written to tf summary.')
and use this function in func repeated_checkpoint_run

./eval_util.py func repeated_checkpoint_run

#write_metrics(metrics, global_step, summary_dir)
write_metrics(metrics=metrics, pr_value=pr_value, global_step=global_step, summary_dir=summary_dir)
The re-run the eval.py, you can get some result like this:
screenshot from 2018-01-17 13-11-09

Cheers,

Let me know if there are some bugs.


you need to add from tensorboard import summary as summary_lib in the beginning of ./eval_util.py.

Besides, the tensorboard should be the newest one to make the pr_curve function work.

If it is necessary, I could make a pull request for this function.



